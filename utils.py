import torch

import math
import warnings
from PIL import Image
import numpy as np


def center_crop_arr(image_arr, image_size):
    """
    Center cropping implementation from ADM.
    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126
    """
    pil_image = Image.fromarray(image_arr)
    while min(*pil_image.size) >= 2 * image_size:
        pil_image = pil_image.resize(
            tuple(x // 2 for x in pil_image.size), resample=Image.BOX
        )

    scale = image_size / min(*pil_image.size)
    pil_image = pil_image.resize(
        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC
    )

    arr = np.array(pil_image)
    crop_y = (arr.shape[0] - image_size) // 2
    crop_x = (arr.shape[1] - image_size) // 2
    return arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size]


def center_crop_pil(pil_image, image_size):
    """
    Center cropping implementation from ADM.
    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126
    """
    while min(*pil_image.size) >= 2 * image_size:
        pil_image = pil_image.resize(
            tuple(x // 2 for x in pil_image.size), resample=Image.BOX
        )

    scale = image_size / min(*pil_image.size)
    pil_image = pil_image.resize(
        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC
    )

    arr = np.array(pil_image)
    crop_y = (arr.shape[0] - image_size) // 2
    crop_x = (arr.shape[1] - image_size) // 2
    return arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size]


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def load_legacy_checkpoints(state_dict, encoder_depth):
    new_state_dict = dict()
    for key, value in state_dict.items():
        if 'decoder_blocks' in key:
            parts =key.split('.')
            new_idx = int(parts[1]) + encoder_depth
            parts[0] = 'blocks'
            parts[1] = str(new_idx)
            new_key = '.'.join(parts)
            new_state_dict[new_key] = value
        else:
            new_state_dict[key] = value
    return new_state_dict


#################################################################################
# spatial normalization
#################################################################################
ALL_SPNORM_METHODS = ["none", "zscore", "zscore_token", "layernorm"]


def zscore_norm(x: torch.Tensor, dim: int = -1, alpha: float = 1.0, eps: float = 1e-6) -> torch.Tensor:
    """
    Functional Z-score normalization: (x - alpha*mean) / std.

    Per-sample normalization: each sample is normalized independently.
    No multi-GPU sync needed since samples are statistically independent.

    Args:
        x: Input tensor [B, T, D]
        dim: Dimension to normalize along
            - dim=-1 (D): per-token normalization, each token normalized independently
            - dim=1 (T): spatial normalization, each feature normalized across tokens
        alpha: Scaling factor for mean subtraction (default 1.0)
        eps: Small constant for numerical stability

    Returns:
        Normalized tensor with same dtype as input
    """
    # Improved stability: Force float32 computation
    input_dtype = x.dtype
    x = x.float()

    # Compute per-sample statistics
    mean = x.mean(dim=dim, keepdim=True)
    std = x.std(dim=dim, keepdim=True)

    result = (x - alpha * mean) / (std + eps)

    return result.to(input_dtype)

class ZScoreNorm(torch.nn.Module):
    """
    Z-score normalization module using zscore_norm.
    
    Args:
        dim: Dimension to normalize along
            - dim=-1 (D): per-token normalization
            - dim=1 (T): per-feature spatial normalization
        alpha: Scaling factor for mean subtraction (default 1.0)
        eps: Small constant for numerical stability
    """
    def __init__(self, dim: int = -1, alpha: float = 1.0, eps: float = 1e-6):
        super().__init__()
        self.dim = dim
        self.alpha = alpha
        self.eps = eps
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return zscore_norm(x, dim=self.dim, alpha=self.alpha, eps=self.eps)
